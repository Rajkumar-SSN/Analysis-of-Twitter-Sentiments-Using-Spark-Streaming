{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSeVn2IzJ9_"
      },
      "source": [
        "## Import modules and create spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf8KoqgU6e6d",
        "outputId": "dd627767-b12b-4360-a526-cc08a2d17e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=4e5b967bc996b866728024d60c47d1c395d6ec6c7055052fdd599e39cbdd4deb\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DoxvW4_bzJ-B"
      },
      "outputs": [],
      "source": [
        "#import modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
        "\n",
        "#create Spark session\n",
        "appName = \"Sentiment Analysis in Spark\"\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(appName) \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXKLer5DzJ-D"
      },
      "source": [
        "## Read data file into Spark dataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXHrxuGJ4cV7",
        "outputId": "f5a79b5f-b3b8-4a15-e1fd-6fa5038f39c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnVuqBknzJ-D",
        "outputId": "f69dbe22-449a-4c09-cbe1-6b3e7913f2b7",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---------------+---------------------------------+\n",
            "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
            "+------+---------+---------------+---------------------------------+\n",
            "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
            "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
            "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
            "+------+---------+---------------+---------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#read csv file into dataFrame with automatically inferred schema\n",
        "tweets_csv = spark.read.csv('/content/tweets_dataset.csv', inferSchema=True, header=True)\n",
        "tweets_csv.show(truncate=False, n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzm4hXhrIXtY",
        "outputId": "536ff269-dbf5-4051-dc34-7e097f8886c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(ItemID=1038, Sentiment=1, SentimentSource='Sentiment140', SentimentText='that film is fantastic #brilliant'),\n",
              " Row(ItemID=1804, Sentiment=1, SentimentSource='Sentiment140', SentimentText='this music is really bad #myband'),\n",
              " Row(ItemID=1693, Sentiment=0, SentimentSource='Sentiment140', SentimentText='winter is terrible #thumbs-down'),\n",
              " Row(ItemID=1477, Sentiment=0, SentimentSource='Sentiment140', SentimentText='this game is awful #nightmare'),\n",
              " Row(ItemID=45, Sentiment=1, SentimentSource='Sentiment140', SentimentText='I love jam #loveit')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tweets_csv.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2XTlu4ZzJ-E"
      },
      "source": [
        "## Select the related data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEsFEzKjzJ-F",
        "outputId": "596b43f5-72a4-452b-8564-c4f4d5cfc902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+-----+\n",
            "|SentimentText                    |label|\n",
            "+---------------------------------+-----+\n",
            "|that film is fantastic #brilliant|1    |\n",
            "|this music is really bad #myband |1    |\n",
            "|winter is terrible #thumbs-down  |0    |\n",
            "|this game is awful #nightmare    |0    |\n",
            "|I love jam #loveit               |1    |\n",
            "+---------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#select only \"SentimentText\" and \"Sentiment\" column,\n",
        "#and cast \"Sentiment\" column data into integer\n",
        "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
        "data.show(truncate = False,n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cIU_RL9zJ-F"
      },
      "source": [
        "## Divide data into training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-D3pkydzJ-G",
        "outputId": "07d79be9-bf8f-4c42-a48b-637926779e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data rows: 1360 ; Testing data rows: 572\n"
          ]
        }
      ],
      "source": [
        "#divide data, 70% for training, 30% for testing\n",
        "dividedData = data.randomSplit([0.7, 0.3])\n",
        "trainingData = dividedData[0] #index 0 = data training\n",
        "testingData = dividedData[1] #index 1 = data testing\n",
        "train_rows = trainingData.count()\n",
        "test_rows = testingData.count()\n",
        "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g3KYiQjzJ-H"
      },
      "source": [
        "## Prepare training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bj9yNtBzJ-H"
      },
      "source": [
        "Separate \"SentimentText\" into individual words using tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jMOfJLGzJ-I",
        "outputId": "a1ec61c3-2b28-4419-ee76-33fefdec8fd9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+-----+----------------------------------------+\n",
            "|SentimentText                     |label|SentimentWords                          |\n",
            "+----------------------------------+-----+----------------------------------------+\n",
            "|I adore cheese #brilliant         |1    |[i, adore, cheese, #brilliant]          |\n",
            "|I adore cheese #toptastic         |1    |[i, adore, cheese, #toptastic]          |\n",
            "|I adore classical music #bestever |1    |[i, adore, classical, music, #bestever] |\n",
            "|I adore classical music #favorite |1    |[i, adore, classical, music, #favorite] |\n",
            "|I adore classical music #thumbs-up|1    |[i, adore, classical, music, #thumbs-up]|\n",
            "+----------------------------------+-----+----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
        "tokenizedTrain = tokenizer.transform(trainingData)\n",
        "tokenizedTrain.show(truncate=False, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59QjJ3TGzJ-I"
      },
      "source": [
        "Removing stop words (unimportant words to be features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL19Okw5zJ-I",
        "outputId": "6c3a1750-c3c4-4b38-9a47-1e7588ab8885",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
            "|SentimentText                     |label|SentimentWords                          |MeaningfulWords                      |\n",
            "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
            "|I adore cheese #brilliant         |1    |[i, adore, cheese, #brilliant]          |[adore, cheese, #brilliant]          |\n",
            "|I adore cheese #toptastic         |1    |[i, adore, cheese, #toptastic]          |[adore, cheese, #toptastic]          |\n",
            "|I adore classical music #bestever |1    |[i, adore, classical, music, #bestever] |[adore, classical, music, #bestever] |\n",
            "|I adore classical music #favorite |1    |[i, adore, classical, music, #favorite] |[adore, classical, music, #favorite] |\n",
            "|I adore classical music #thumbs-up|1    |[i, adore, classical, music, #thumbs-up]|[adore, classical, music, #thumbs-up]|\n",
            "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol=\"MeaningfulWords\")\n",
        "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
        "SwRemovedTrain.show(truncate=False, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnUkXOPyzJ-J"
      },
      "source": [
        "Converting words feature into numerical feature. In Spark 2.2.1,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "countVectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
        "countVectorizerModel = countVectorizer.fit(SwRemovedTrain)\n",
        "numericTrainData = countVectorizerModel.transform(SwRemovedTrain).select('label', 'MeaningfulWords', 'features')\n",
        "numericTrainData.show(truncate=False, n=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diRYjq8If1gg",
        "outputId": "63ed5348-791b-4355-f317-1d19235e9b3e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------------------+-----------------------------------+\n",
            "|label|MeaningfulWords                     |features                           |\n",
            "+-----+------------------------------------+-----------------------------------+\n",
            "|1    |[adore, cheese, #brilliant]         |(48,[14,23,39],[1.0,1.0,1.0])      |\n",
            "|1    |[adore, cheese, #toptastic]         |(48,[13,23,39],[1.0,1.0,1.0])      |\n",
            "|1    |[adore, classical, music, #bestever]|(48,[0,17,23,43],[1.0,1.0,1.0,1.0])|\n",
            "+-----+------------------------------------+-----------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer is a feature extraction technique commonly used in natural language processing (NLP) and machine learning for converting a collection of text documents into a numerical feature vector. The basic idea behind CountVectorizer is to represent each document as a vector of term frequencies, indicating how often each term (word) appears in the document.\n",
        "\n"
      ],
      "metadata": {
        "id": "CAZVp_8JlpxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "hashingTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=10)\n",
        "featurizedData = hashingTF.transform(SwRemovedTrain)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "numericTrainData = idfModel.transform(featurizedData).select('label', 'MeaningfulWords', 'features')\n",
        "numericTrainData.show(truncate=False, n=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IP7Nnx9iIuq",
        "outputId": "8b4a50aa-5bb8-46f8-a95a-4040ad19d95e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "|label|MeaningfulWords                     |features                                                                                    |\n",
            "+-----+------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "|1    |[adore, cheese, #brilliant]         |(10,[1,3,5],[1.6138561817717652,0.8041567349415689,1.2837298152034553])                     |\n",
            "|1    |[adore, cheese, #toptastic]         |(10,[3,4,5],[0.8041567349415689,1.3988638426882618,1.2837298152034553])                     |\n",
            "|1    |[adore, classical, music, #bestever]|(10,[0,1,5,7],[0.6493025728482252,1.6138561817717652,1.2837298152034553,1.2523956590330196])|\n",
            "+-----+------------------------------------+--------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency). This is a vectorization where it shows how relevant a word is to a document in a collection of documents. It is often used in information retrieval and text mining. For this we need HashTF.\n"
      ],
      "metadata": {
        "id": "dCDU5b-mkXp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer, PCA\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "countVectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
        "pca = PCA(k=5, inputCol=\"features\", outputCol=\"pca_features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[countVectorizer, pca])\n",
        "model = pipeline.fit(SwRemovedTrain)\n",
        "numericTrainData = model.transform(SwRemovedTrain).select('label', 'MeaningfulWords', 'pca_features')\n",
        "\n",
        "numericTrainData.show(truncate=False, n=3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "253TFwk7iWn8",
        "outputId": "598e8bf7-2dbf-4591-a285-0f63e483252b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
            "|label|MeaningfulWords                     |pca_features                                                                                           |\n",
            "+-----+------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
            "|1    |[adore, cheese, #brilliant]         |[-0.3067392965026029,0.11863761241711032,0.23348234054443562,-0.08554811256824954,-0.2331803288141199] |\n",
            "|1    |[adore, cheese, #toptastic]         |[-0.3198324346870657,0.06925904412739661,0.24410500995423473,-0.0648631679881858,-0.010678835876804271]|\n",
            "|1    |[adore, classical, music, #bestever]|[-0.5178150571374159,-1.0272791377888453,0.20433782921131646,-0.1340323524389118,0.09752765410588701]  |\n",
            "+-----+------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA (Principal Component Analysis):\n",
        "PCA is a dimensionality reduction vectorization technique that can be applied to reduce the number of features in your dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "V3VsNmhNkpSm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T13jgO7QzJ-J",
        "outputId": "b7f96ac5-3af5-4bb8-b3c1-e4c9a2c08905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------------------+-------------------------------------------------------+\n",
            "|label|MeaningfulWords                     |features                                               |\n",
            "+-----+------------------------------------+-------------------------------------------------------+\n",
            "|1    |[adore, cheese, #brilliant]         |(262144,[1689,45361,100089],[1.0,1.0,1.0])             |\n",
            "|1    |[adore, cheese, #toptastic]         |(262144,[1689,42010,100089],[1.0,1.0,1.0])             |\n",
            "|1    |[adore, classical, music, #bestever]|(262144,[91011,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
            "+-----+------------------------------------+-------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
        "numericTrainData = hashTF.transform(SwRemovedTrain).select('label', 'MeaningfulWords', 'features')\n",
        "numericTrainData.show(truncate=False, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQUxJZ4LzJ-J"
      },
      "source": [
        "## Train our classifier model using training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n",
        "model = lr.fit(numericTrainData)\n",
        "print(\"Training is done!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmAkMrfbPYlG",
        "outputId": "44bbae30-bcfa-4568-b878-dfa52f8a8738"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KLfPkh4zJ-J",
        "outputId": "629204cb-5070-4baf-f3ca-3da527661e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is done!\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",maxIter=10, regParam=0.01)\n",
        "model = lr.fit(numericTrainData)\n",
        "print (\"Training is done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sLd4i5CP6eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i3FpJCezJ-K"
      },
      "source": [
        "## Prepare testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIU7hkwTzJ-K",
        "outputId": "5dc73cc8-53ba-4fe9-d8f6-8db955fd48d4",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------------------+------------------------------------------+\n",
            "|Label|MeaningfulWords            |features                                  |\n",
            "+-----+---------------------------+------------------------------------------+\n",
            "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0])|\n",
            "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0])|\n",
            "+-----+---------------------------+------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizedTest = tokenizer.transform(testingData)\n",
        "SwRemovedTest = swr.transform(tokenizedTest)\n",
        "numericTest = hashTF.transform(SwRemovedTest).select('Label', 'MeaningfulWords', 'features')\n",
        "numericTest.show(truncate=False, n=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUS27St-zJ-K"
      },
      "source": [
        "## Predict testing data and calculate the accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miSGQJKXzJ-L",
        "outputId": "d03d782b-0a7f-49d1-be6a-3d451aafdbff",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------+----------+-----+\n",
            "|MeaningfulWords                      |prediction|Label|\n",
            "+-------------------------------------+----------+-----+\n",
            "|[adore, cheese, #bestever]           |1.0       |1    |\n",
            "|[adore, cheese, #brilliant]          |1.0       |1    |\n",
            "|[adore, classical, music, #bestever] |1.0       |1    |\n",
            "|[adore, classical, music, #brilliant]|1.0       |1    |\n",
            "+-------------------------------------+----------+-----+\n",
            "only showing top 4 rows\n",
            "\n",
            "correct prediction: 562 , total data: 571 , accuracy: 0.9842381786339754\n"
          ]
        }
      ],
      "source": [
        "prediction = model.transform(numericTest)\n",
        "predictionFinal = prediction.select(\"MeaningfulWords\", \"prediction\", \"Label\")\n",
        "predictionFinal.show(n=4, truncate = False)\n",
        "correctPrediction = predictionFinal.filter(predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
        "totalData = predictionFinal.count()\n",
        "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData,\", accuracy:\", correctPrediction/totalData)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}